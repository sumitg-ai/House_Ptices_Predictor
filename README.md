Local MLOps API with Docker + Terraform
ğŸ“Œ Overview
This project demonstrates a simple MLOps pipeline using:
â€¢	A LinearRegression model trained on the California Housing dataset
â€¢	A Flask API to serve predictions
â€¢	Docker to containerize the app
â€¢	Terraform to manage the local Docker container infrastructure
________________________________________
ğŸ“ Project Structure
.
â”œâ”€â”€ app.py               # Flask API serving predictions
â”œâ”€â”€ train.py             # Trains and saves model.joblib
â”œâ”€â”€ model/               # Folder where trained model is saved
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ Dockerfile           # Builds image & runs train.py
â”œâ”€â”€ .gitignore
â””â”€â”€ terraform/
    â”œâ”€â”€ main.tf          # Terraform config to spin up Docker container locally
    â””â”€â”€ outputs.tf       # Useful outputs like container name and API URL
________________________________________
ğŸ›  Prerequisites
â€¢	Docker
â€¢	Terraform
________________________________________
ğŸš€ Step-by-Step Instructions
âœ… 1. Clone the Repo
git clone https://github.com/yourusername/mlops-flask-api.git
cd mlops-flask-api
âœ… 2. Build the Docker Image (Handled by Terraform)
No need to run docker build manually â€” Terraform handles it.
âœ… 3. Initialize Terraform
cd terraform
terraform init
âœ… 4. Apply Terraform to Start the App
terraform apply
â€¢	This will:
o	Build the Docker image
o	Train the ML model inside the image
o	Start a container running the Flask app
o	Expose the app at http://localhost:5000
________________________________________
ğŸ“¡ API Usage
ğŸ”— Endpoint
POST http://localhost:5000/predict
ğŸ§¾ Example Request Body
{
  "features": [8.3252, 41, 6.9841, 1.0238, 322, 2.5556, 37.88, -122.23]
}
Replace the array values with values matching the California Housing dataset format.
âœ… Example Response
{
  "prediction": [4.526]
}
________________________________________
ğŸ“¤ Terraform Outputs
After terraform apply, youâ€™ll see:
Outputs:

container_name = "mlops-api-container"
host_port = 5000
url = "http://localhost:5000/predict"
________________________________________
ğŸ§¹ Cleanup
To stop and remove the container and image:
terraform destroy
________________________________________
ğŸ“¦ Dockerfile Used
FROM python:3.9-slim

WORKDIR /app
COPY . /app

RUN pip install --no-cache-dir -r requirements.txt
RUN python train.py

EXPOSE 5000
CMD ["python", "app.py"]
________________________________________
âœ… Next Steps
â€¢	Add metrics logging
â€¢	Serve with a model versioning strategy
â€¢	Push Docker image to DockerHub or AWS ECR
â€¢	Expand to AWS EC2 using Terraform
________________________________________
Let me know if you want this converted into a downloadable Markdown file or want badges (like Docker build status, Python version, etc.) added.

